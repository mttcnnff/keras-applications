{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mttcnnff/keras-applications/blob/master/mattsmbv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctaFWBQK0rTT",
        "colab_type": "code",
        "outputId": "5a390fec-39b9-429a-8977-8eb7f16ea0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!rm -rf keras-applications\n",
        "!rm *.*"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '*.*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyQms18Izy8_",
        "colab_type": "code",
        "outputId": "d8476838-d0d4-4c21-b4d9-8c16352b7d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/mttcnnff/keras-applications.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-applications'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects:  12% (1/8)   \u001b[K\rremote: Counting objects:  25% (2/8)   \u001b[K\rremote: Counting objects:  37% (3/8)   \u001b[K\rremote: Counting objects:  50% (4/8)   \u001b[K\rremote: Counting objects:  62% (5/8)   \u001b[K\rremote: Counting objects:  75% (6/8)   \u001b[K\rremote: Counting objects:  87% (7/8)   \u001b[K\rremote: Counting objects: 100% (8/8)   \u001b[K\rremote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects:  12% (1/8)   \u001b[K\rremote: Compressing objects:  25% (2/8)   \u001b[K\rremote: Compressing objects:  37% (3/8)   \u001b[K\rremote: Compressing objects:  50% (4/8)   \u001b[K\rremote: Compressing objects:  62% (5/8)   \u001b[K\rremote: Compressing objects:  75% (6/8)   \u001b[K\rremote: Compressing objects:  87% (7/8)   \u001b[K\rremote: Compressing objects: 100% (8/8)   \u001b[K\rremote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "Receiving objects:   0% (1/461)   \rReceiving objects:   1% (5/461)   \rReceiving objects:   2% (10/461)   \rReceiving objects:   3% (14/461)   \rReceiving objects:   4% (19/461)   \rReceiving objects:   5% (24/461)   \rReceiving objects:   6% (28/461)   \rReceiving objects:   7% (33/461)   \rReceiving objects:   8% (37/461)   \rReceiving objects:   9% (42/461)   \rReceiving objects:  10% (47/461)   \rReceiving objects:  11% (51/461)   \rReceiving objects:  12% (56/461)   \rReceiving objects:  13% (60/461)   \rReceiving objects:  14% (65/461)   \rReceiving objects:  15% (70/461)   \rReceiving objects:  16% (74/461)   \rReceiving objects:  17% (79/461)   \rReceiving objects:  18% (83/461)   \rReceiving objects:  19% (88/461)   \rReceiving objects:  20% (93/461)   \rReceiving objects:  21% (97/461)   \rReceiving objects:  22% (102/461)   \rReceiving objects:  23% (107/461)   \rReceiving objects:  24% (111/461)   \rReceiving objects:  25% (116/461)   \rReceiving objects:  26% (120/461)   \rReceiving objects:  27% (125/461)   \rReceiving objects:  28% (130/461)   \rReceiving objects:  29% (134/461)   \rReceiving objects:  30% (139/461)   \rReceiving objects:  31% (143/461)   \rReceiving objects:  32% (148/461)   \rReceiving objects:  33% (153/461)   \rReceiving objects:  34% (157/461)   \rReceiving objects:  35% (162/461)   \rReceiving objects:  36% (166/461)   \rReceiving objects:  37% (171/461)   \rReceiving objects:  38% (176/461)   \rReceiving objects:  39% (180/461)   \rReceiving objects:  40% (185/461)   \rReceiving objects:  41% (190/461)   \rReceiving objects:  42% (194/461)   \rReceiving objects:  43% (199/461)   \rReceiving objects:  44% (203/461)   \rReceiving objects:  45% (208/461)   \rReceiving objects:  46% (213/461)   \rReceiving objects:  47% (217/461)   \rReceiving objects:  48% (222/461)   \rReceiving objects:  49% (226/461)   \rReceiving objects:  50% (231/461)   \rReceiving objects:  51% (236/461)   \rReceiving objects:  52% (240/461)   \rReceiving objects:  53% (245/461)   \rReceiving objects:  54% (249/461)   \rReceiving objects:  55% (254/461)   \rReceiving objects:  56% (259/461)   \rReceiving objects:  57% (263/461)   \rReceiving objects:  58% (268/461)   \rReceiving objects:  59% (272/461)   \rReceiving objects:  60% (277/461)   \rReceiving objects:  61% (282/461)   \rReceiving objects:  62% (286/461)   \rReceiving objects:  63% (291/461)   \rReceiving objects:  64% (296/461)   \rReceiving objects:  65% (300/461)   \rReceiving objects:  66% (305/461)   \rReceiving objects:  67% (309/461)   \rReceiving objects:  68% (314/461)   \rReceiving objects:  69% (319/461)   \rReceiving objects:  70% (323/461)   \rReceiving objects:  71% (328/461)   \rReceiving objects:  72% (332/461)   \rReceiving objects:  73% (337/461)   \rReceiving objects:  74% (342/461)   \rReceiving objects:  75% (346/461)   \rReceiving objects:  76% (351/461)   \rReceiving objects:  77% (355/461)   \rReceiving objects:  78% (360/461)   \rReceiving objects:  79% (365/461)   \rReceiving objects:  80% (369/461)   \rReceiving objects:  81% (374/461)   \rReceiving objects:  82% (379/461)   \rremote: Total 461 (delta 2), reused 0 (delta 0), pack-reused 453\u001b[K\n",
            "Receiving objects:  83% (383/461)   \rReceiving objects:  84% (388/461)   \rReceiving objects:  85% (392/461)   \rReceiving objects:  86% (397/461)   \rReceiving objects:  87% (402/461)   \rReceiving objects:  88% (406/461)   \rReceiving objects:  89% (411/461)   \rReceiving objects:  90% (415/461)   \rReceiving objects:  91% (420/461)   \rReceiving objects:  92% (425/461)   \rReceiving objects:  93% (429/461)   \rReceiving objects:  94% (434/461)   \rReceiving objects:  95% (438/461)   \rReceiving objects:  96% (443/461)   \rReceiving objects:  97% (448/461)   \rReceiving objects:  98% (452/461)   \rReceiving objects:  99% (457/461)   \rReceiving objects: 100% (461/461)   \rReceiving objects: 100% (461/461), 447.85 KiB | 5.74 MiB/s, done.\n",
            "Resolving deltas:   0% (0/326)   \rResolving deltas:   3% (10/326)   \rResolving deltas:   4% (14/326)   \rResolving deltas:   6% (20/326)   \rResolving deltas:  11% (36/326)   \rResolving deltas:  28% (92/326)   \rResolving deltas:  37% (121/326)   \rResolving deltas:  38% (124/326)   \rResolving deltas:  48% (157/326)   \rResolving deltas:  50% (163/326)   \rResolving deltas:  53% (175/326)   \rResolving deltas:  54% (177/326)   \rResolving deltas:  55% (180/326)   \rResolving deltas:  80% (261/326)   \rResolving deltas:  82% (268/326)   \rResolving deltas:  84% (277/326)   \rResolving deltas:  86% (282/326)   \rResolving deltas:  88% (287/326)   \rResolving deltas:  89% (291/326)   \rResolving deltas:  91% (299/326)   \rResolving deltas:  92% (303/326)   \rResolving deltas:  95% (311/326)   \rResolving deltas:  96% (313/326)   \rResolving deltas:  98% (321/326)   \rResolving deltas:  99% (324/326)   \rResolving deltas: 100% (326/326)   \rResolving deltas: 100% (326/326), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNQcC_IwTD4I",
        "colab_type": "code",
        "outputId": "5d04f4a5-3eca-4aa8-81d6-866cf5574cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mv keras-applications/keras_applications ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot move 'keras-applications/keras_applications' to './keras_applications': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twVJQism0Gk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhJPi9s5z5pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"MobileNet v2 models for Keras.\n",
        "\n",
        "MobileNetV2 is a general architecture and can be used for multiple use cases.\n",
        "Depending on the use case, it can use different input layer size and\n",
        "different width factors. This allows different width models to reduce\n",
        "the number of multiply-adds and thereby\n",
        "reduce inference cost on mobile devices.\n",
        "\n",
        "MobileNetV2 is very similar to the original MobileNet,\n",
        "except that it uses inverted residual blocks with\n",
        "bottlenecking features. It has a drastically lower\n",
        "parameter count than the original MobileNet.\n",
        "MobileNets support any input size greater\n",
        "than 32 x 32, with larger image sizes\n",
        "offering better performance.\n",
        "\n",
        "The number of parameters and number of multiply-adds\n",
        "can be modified by using the `alpha` parameter,\n",
        "which increases/decreases the number of filters in each layer.\n",
        "By altering the image size and `alpha` parameter,\n",
        "all 22 models from the paper can be built, with ImageNet weights provided.\n",
        "\n",
        "The paper demonstrates the performance of MobileNets using `alpha` values of\n",
        "1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4\n",
        "\n",
        "For each of these `alpha` values, weights for 5 different input image sizes\n",
        "are provided (224, 192, 160, 128, and 96).\n",
        "\n",
        "\n",
        "The following table describes the performance of\n",
        "MobileNet on various input sizes:\n",
        "------------------------------------------------------------------------\n",
        "MACs stands for Multiply Adds\n",
        "\n",
        " Classification Checkpoint| MACs (M) | Parameters (M)| Top 1 Accuracy| Top 5 Accuracy\n",
        "--------------------------|------------|---------------|---------|----|-------------\n",
        "| [mobilenet_v2_1.4_224]  | 582 | 6.06 |          75.0 | 92.5 |\n",
        "| [mobilenet_v2_1.3_224]  | 509 | 5.34 |          74.4 | 92.1 |\n",
        "| [mobilenet_v2_1.0_224]  | 300 | 3.47 |          71.8 | 91.0 |\n",
        "| [mobilenet_v2_1.0_192]  | 221 | 3.47 |          70.7 | 90.1 |\n",
        "| [mobilenet_v2_1.0_160]  | 154 | 3.47 |          68.8 | 89.0 |\n",
        "| [mobilenet_v2_1.0_128]  | 99  | 3.47 |          65.3 | 86.9 |\n",
        "| [mobilenet_v2_1.0_96]   | 56  | 3.47 |          60.3 | 83.2 |\n",
        "| [mobilenet_v2_0.75_224] | 209 | 2.61 |          69.8 | 89.6 |\n",
        "| [mobilenet_v2_0.75_192] | 153 | 2.61 |          68.7 | 88.9 |\n",
        "| [mobilenet_v2_0.75_160] | 107 | 2.61 |          66.4 | 87.3 |\n",
        "| [mobilenet_v2_0.75_128] | 69  | 2.61 |          63.2 | 85.3 |\n",
        "| [mobilenet_v2_0.75_96]  | 39  | 2.61 |          58.8 | 81.6 |\n",
        "| [mobilenet_v2_0.5_224]  | 97  | 1.95 |          65.4 | 86.4 |\n",
        "| [mobilenet_v2_0.5_192]  | 71  | 1.95 |          63.9 | 85.4 |\n",
        "| [mobilenet_v2_0.5_160]  | 50  | 1.95 |          61.0 | 83.2 |\n",
        "| [mobilenet_v2_0.5_128]  | 32  | 1.95 |          57.7 | 80.8 |\n",
        "| [mobilenet_v2_0.5_96]   | 18  | 1.95 |          51.2 | 75.8 |\n",
        "| [mobilenet_v2_0.35_224] | 59  | 1.66 |          60.3 | 82.9 |\n",
        "| [mobilenet_v2_0.35_192] | 43  | 1.66 |          58.2 | 81.2 |\n",
        "| [mobilenet_v2_0.35_160] | 30  | 1.66 |          55.7 | 79.1 |\n",
        "| [mobilenet_v2_0.35_128] | 20  | 1.66 |          50.8 | 75.0 |\n",
        "| [mobilenet_v2_0.35_96]  | 11  | 1.66 |          45.5 | 70.4 |\n",
        "\n",
        "The weights for all 16 models are obtained and\n",
        "translated from the Tensorflow checkpoints\n",
        "from TensorFlow checkpoints found [here]\n",
        "(https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md).\n",
        "\n",
        "# Reference\n",
        "\n",
        "This file contains building code for MobileNetV2, based on\n",
        "[MobileNetV2: Inverted Residuals and Linear Bottlenecks]\n",
        "(https://arxiv.org/abs/1801.04381) (CVPR 2018)\n",
        "\n",
        "Tests comparing this model to the existing Tensorflow model can be\n",
        "found at [mobilenet_v2_keras]\n",
        "(https://github.com/JonathanCMitchell/mobilenet_v2_keras)\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "from keras_applications import correct_pad\n",
        "from keras_applications import get_submodules_from_kwargs\n",
        "from keras_applications.imagenet_utils import decode_predictions\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "\n",
        "# TODO Change path to v1.1\n",
        "BASE_WEIGHT_PATH = ('https://github.com/JonathanCMitchell/mobilenet_v2_keras/'\n",
        "                    'releases/download/v1.1/')\n",
        "\n",
        "backend = None\n",
        "layers = None\n",
        "models = None\n",
        "keras_utils = None\n",
        "\n",
        "\n",
        "def preprocess_input(x, **kwargs):\n",
        "    \"\"\"Preprocesses a numpy array encoding a batch of images.\n",
        "\n",
        "    # Arguments\n",
        "        x: a 4D numpy array consists of RGB values within [0, 255].\n",
        "\n",
        "    # Returns\n",
        "        Preprocessed array.\n",
        "    \"\"\"\n",
        "    return imagenet_utils.preprocess_input(x, mode='tf', **kwargs)\n",
        "\n",
        "\n",
        "# This function is taken from the original tf repo.\n",
        "# It ensures that all layers have a channel number that is divisible by 8\n",
        "# It can be seen here:\n",
        "# https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "def MobileNetV2(input_shape=None,\n",
        "                alpha=1.0,\n",
        "                include_top=True,\n",
        "                weights='imagenet',\n",
        "                input_tensor=None,\n",
        "                pooling=None,\n",
        "                classes=1000,\n",
        "                **kwargs):\n",
        "    \"\"\"Instantiates the MobileNetV2 architecture.\n",
        "\n",
        "    # Arguments\n",
        "        input_shape: optional shape tuple, to be specified if you would\n",
        "            like to use a model with an input img resolution that is not\n",
        "            (224, 224, 3).\n",
        "            It should have exactly 3 inputs channels (224, 224, 3).\n",
        "            You can also omit this option if you would like\n",
        "            to infer input_shape from an input_tensor.\n",
        "            If you choose to include both input_tensor and input_shape then\n",
        "            input_shape will be used if they match, if the shapes\n",
        "            do not match then we will throw an error.\n",
        "            E.g. `(160, 160, 3)` would be one valid value.\n",
        "        alpha: controls the width of the network. This is known as the\n",
        "        width multiplier in the MobileNetV2 paper, but the name is kept for\n",
        "        consistency with MobileNetV1 in Keras.\n",
        "            - If `alpha` < 1.0, proportionally decreases the number\n",
        "                of filters in each layer.\n",
        "            - If `alpha` > 1.0, proportionally increases the number\n",
        "                of filters in each layer.\n",
        "            - If `alpha` = 1, default number of filters from the paper\n",
        "                 are used at each layer.\n",
        "        include_top: whether to include the fully-connected\n",
        "            layer at the top of the network.\n",
        "        weights: one of `None` (random initialization),\n",
        "              'imagenet' (pre-training on ImageNet),\n",
        "              or the path to the weights file to be loaded.\n",
        "        input_tensor: optional Keras tensor (i.e. output of\n",
        "            `layers.Input()`)\n",
        "            to use as image input for the model.\n",
        "        pooling: Optional pooling mode for feature extraction\n",
        "            when `include_top` is `False`.\n",
        "            - `None` means that the output of the model\n",
        "                will be the 4D tensor output of the\n",
        "                last convolutional block.\n",
        "            - `avg` means that global average pooling\n",
        "                will be applied to the output of the\n",
        "                last convolutional block, and thus\n",
        "                the output of the model will be a\n",
        "                2D tensor.\n",
        "            - `max` means that global max pooling will\n",
        "                be applied.\n",
        "        classes: optional number of classes to classify images\n",
        "            into, only to be specified if `include_top` is True, and\n",
        "            if no `weights` argument is specified.\n",
        "\n",
        "    # Returns\n",
        "        A Keras model instance.\n",
        "\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument for `weights`,\n",
        "            or invalid input shape or invalid alpha, rows when\n",
        "            weights='imagenet'\n",
        "    \"\"\"\n",
        "    global backend, layers, models, keras_utils\n",
        "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization), `imagenet` '\n",
        "                         '(pre-training on ImageNet), '\n",
        "                         'or the path to the weights file to be loaded.')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top` '\n",
        "                         'as true, `classes` should be 1000')\n",
        "\n",
        "    # Determine proper input shape and default size.\n",
        "    # If both input_shape and input_tensor are used, they should match\n",
        "    if input_shape is not None and input_tensor is not None:\n",
        "        try:\n",
        "            is_input_t_tensor = backend.is_keras_tensor(input_tensor)\n",
        "        except ValueError:\n",
        "            try:\n",
        "                is_input_t_tensor = backend.is_keras_tensor(\n",
        "                    keras_utils.get_source_inputs(input_tensor))\n",
        "            except ValueError:\n",
        "                raise ValueError('input_tensor: ', input_tensor,\n",
        "                                 'is not type input_tensor')\n",
        "        if is_input_t_tensor:\n",
        "            if backend.image_data_format == 'channels_first':\n",
        "                if backend.int_shape(input_tensor)[1] != input_shape[1]:\n",
        "                    raise ValueError('input_shape: ', input_shape,\n",
        "                                     'and input_tensor: ', input_tensor,\n",
        "                                     'do not meet the same shape requirements')\n",
        "            else:\n",
        "                if backend.int_shape(input_tensor)[2] != input_shape[1]:\n",
        "                    raise ValueError('input_shape: ', input_shape,\n",
        "                                     'and input_tensor: ', input_tensor,\n",
        "                                     'do not meet the same shape requirements')\n",
        "        else:\n",
        "            raise ValueError('input_tensor specified: ', input_tensor,\n",
        "                             'is not a keras tensor')\n",
        "\n",
        "    # If input_shape is None, infer shape from input_tensor\n",
        "    if input_shape is None and input_tensor is not None:\n",
        "\n",
        "        try:\n",
        "            backend.is_keras_tensor(input_tensor)\n",
        "        except ValueError:\n",
        "            raise ValueError('input_tensor: ', input_tensor,\n",
        "                             'is type: ', type(input_tensor),\n",
        "                             'which is not a valid type')\n",
        "\n",
        "        if input_shape is None and not backend.is_keras_tensor(input_tensor):\n",
        "            default_size = 224\n",
        "        elif input_shape is None and backend.is_keras_tensor(input_tensor):\n",
        "            if backend.image_data_format() == 'channels_first':\n",
        "                rows = backend.int_shape(input_tensor)[2]\n",
        "                cols = backend.int_shape(input_tensor)[3]\n",
        "            else:\n",
        "                rows = backend.int_shape(input_tensor)[1]\n",
        "                cols = backend.int_shape(input_tensor)[2]\n",
        "\n",
        "            if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
        "                default_size = rows\n",
        "            else:\n",
        "                default_size = 224\n",
        "\n",
        "    # If input_shape is None and no input_tensor\n",
        "    elif input_shape is None:\n",
        "        default_size = 224\n",
        "\n",
        "    # If input_shape is not None, assume default size\n",
        "    else:\n",
        "        if backend.image_data_format() == 'channels_first':\n",
        "            rows = input_shape[1]\n",
        "            cols = input_shape[2]\n",
        "        else:\n",
        "            rows = input_shape[0]\n",
        "            cols = input_shape[1]\n",
        "\n",
        "        if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
        "            default_size = rows\n",
        "        else:\n",
        "            default_size = 224\n",
        "\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=default_size,\n",
        "                                      min_size=32,\n",
        "                                      data_format=backend.image_data_format(),\n",
        "                                      require_flatten=include_top,\n",
        "                                      weights=weights)\n",
        "\n",
        "    if backend.image_data_format() == 'channels_last':\n",
        "        row_axis, col_axis = (0, 1)\n",
        "    else:\n",
        "        row_axis, col_axis = (1, 2)\n",
        "    rows = input_shape[row_axis]\n",
        "    cols = input_shape[col_axis]\n",
        "\n",
        "    if weights == 'imagenet':\n",
        "        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:\n",
        "            raise ValueError('If imagenet weights are being loaded, '\n",
        "                             'alpha can be one of `0.35`, `0.50`, `0.75`, '\n",
        "                             '`1.0`, `1.3` or `1.4` only.')\n",
        "\n",
        "        if rows != cols or rows not in [96, 128, 160, 192, 224]:\n",
        "            rows = 224\n",
        "            warnings.warn('`input_shape` is undefined or non-square, '\n",
        "                          'or `rows` is not in [96, 128, 160, 192, 224].'\n",
        "                          ' Weights for input shape (224, 224) will be'\n",
        "                          ' loaded as the default.')\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = layers.Input(shape=input_shape)\n",
        "    else:\n",
        "        if not backend.is_keras_tensor(input_tensor):\n",
        "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
        "    x = layers.ZeroPadding2D(padding=correct_pad(backend, img_input, 3),\n",
        "                             name='Conv1_pad')(img_input)\n",
        "    x = layers.Conv2D(first_block_filters,\n",
        "                      kernel_size=3,\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      use_bias=False,\n",
        "                      name='Conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name='bn_Conv1')(x)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
        "                            expansion=1, block_id=0)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=2)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=5)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=6)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=7)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=8)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=9)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=10)\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=11)\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=12)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=13)\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=14)\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=15)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=16)\n",
        "\n",
        "    # no alpha applied to last conv as stated in the paper:\n",
        "    # if the width multiplier is greater than 1 we\n",
        "    # increase the number of output channels\n",
        "    if alpha > 1.0:\n",
        "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
        "    else:\n",
        "        last_block_filters = 1280\n",
        "\n",
        "    x = layers.Conv2D(last_block_filters,\n",
        "                      kernel_size=1,\n",
        "                      use_bias=False,\n",
        "                      name='Conv_1')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name='Conv_1_bn')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "\n",
        "    if include_top:\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(classes, activation='softmax',\n",
        "                         use_bias=True, name='Logits')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = layers.GlobalAveragePooling2D()(x)\n",
        "        elif pooling == 'max':\n",
        "            x = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = models.Model(inputs, x,\n",
        "                         name='mobilenetv2_%0.2f_%s' % (alpha, rows))\n",
        "\n",
        "    # Load weights.\n",
        "    if weights == 'imagenet':\n",
        "        if include_top:\n",
        "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n",
        "                          str(alpha) + '_' + str(rows) + '.h5')\n",
        "            weight_path = BASE_WEIGHT_PATH + model_name\n",
        "            weights_path = keras_utils.get_file(\n",
        "                model_name, weight_path, cache_subdir='models')\n",
        "        else:\n",
        "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n",
        "                          str(alpha) + '_' + str(rows) + '_no_top' + '.h5')\n",
        "            weight_path = BASE_WEIGHT_PATH + model_name\n",
        "            weights_path = keras_utils.get_file(\n",
        "                model_name, weight_path, cache_subdir='models')\n",
        "        model.load_weights(weights_path)\n",
        "    elif weights is not None:\n",
        "        model.load_weights(weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    in_channels = backend.int_shape(inputs)[channel_axis]\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    if block_id:\n",
        "        # Expand\n",
        "        x = layers.Conv2D(expansion * in_channels,\n",
        "                          kernel_size=1,\n",
        "                          padding='same',\n",
        "                          use_bias=False,\n",
        "                          activation=None,\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                      epsilon=1e-3,\n",
        "                                      momentum=0.999,\n",
        "                                      name=prefix + 'expand_BN')(x)\n",
        "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "    # Depthwise\n",
        "    if stride == 2:\n",
        "        x = layers.ZeroPadding2D(padding=correct_pad(backend, x, 3),\n",
        "                                 name=prefix + 'pad')(x)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation=None,\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name=prefix + 'depthwise_BN')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation=None,\n",
        "                      name=prefix + 'project')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name=prefix + 'project_BN')(x)\n",
        "\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDf1G53XedPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(batch):\n",
        "  num_classes = 100\n",
        "  \n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')\n",
        "  y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "  y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      featurewise_center=True,\n",
        "      featurewise_std_normalization=True,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True)\n",
        "\n",
        "  # compute quantities required for featurewise normalization\n",
        "  # (std, mean, and principal components if ZCA whitening is applied)\n",
        "  datagen.fit(x_train)\n",
        "  \n",
        "  train_generator = datagen.flow(x_train, y_train, batch_size=batch)\n",
        "  steps_per_epoch = len(x_train) / batch\n",
        "  \n",
        "  return train_generator, steps_per_epoch\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BECcocg_YqEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# epochs = 10\n",
        "\n",
        "# model = MobileNetV2(input_shape=[32,32,3],\n",
        "#                 alpha=1.0,\n",
        "#                 include_top=True,\n",
        "#                 weights=None,\n",
        "#                 input_tensor=None,\n",
        "#                 pooling=None,\n",
        "#                 classes=100,\n",
        "#                 backend = tf.keras.backend,\n",
        "#                 layers = tf.keras.layers,\n",
        "#                 models = tf.keras.models,\n",
        "#                 utils = tf.keras.utils)\n",
        "\n",
        "# # opt = Adam()\n",
        "# # earlystop = EarlyStopping(monitor='val_acc', patience=30, verbose=0, mode='auto')\n",
        "# # model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "# model.compile(\n",
        "#       optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "#       loss='categorical_crossentropy',\n",
        "#       metrics=['accuracy'])\n",
        "\n",
        "# train_generator, steps_per_epoch = generate(32)\n",
        "\n",
        "# hist = model.fit_generator(\n",
        "#         train_generator,\n",
        "#         steps_per_epoch=steps_per_epoch,\n",
        "#         epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq-nvpDAmFXi",
        "colab_type": "code",
        "outputId": "39b9588c-8fcd-43a3-e923-2f6a3df26fa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 100)\n",
        "x_train = x_train[:49152]\n",
        "y_train = y_train[:49152]\n",
        "\n",
        "x_val = x_train[-9216:]\n",
        "y_val = y_train[-9216:]\n",
        "x_train = x_train[:-9216]\n",
        "y_train = y_train[:-9216]\n",
        "\n",
        "\n",
        "print(len(x_train))\n",
        "\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "  model = MobileNetV2(input_shape=[32,32,3],\n",
        "                    alpha=.5,\n",
        "                    include_top=True,\n",
        "                    weights=None,\n",
        "                    input_tensor=None,\n",
        "                    pooling=None,\n",
        "                    classes=100,\n",
        "                    backend = tf.keras.backend,\n",
        "                    layers = tf.keras.layers,\n",
        "                    models = tf.keras.models,\n",
        "                    utils = tf.keras.utils)\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0719 19:34:36.625512 140547230496640 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0719 19:34:42.997500 140547230496640 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "39936\n",
            "Model: \"mobilenetv2_0.50_32\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_pad (ZeroPadding2D)       (None, 33, 33, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 16, 16, 16)   432         Conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_Conv1 (BatchNormalization)   (None, 16, 16, 16)   64          Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_relu (ReLU)               (None, 16, 16, 16)   0           bn_Conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise (Depthw (None, 16, 16, 16)   144         Conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_BN (Bat (None, 16, 16, 16)   64          expanded_conv_depthwise[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_relu (R (None, 16, 16, 16)   0           expanded_conv_depthwise_BN[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project (Conv2D)  (None, 16, 16, 8)    128         expanded_conv_depthwise_relu[0][0\n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project_BN (Batch (None, 16, 16, 8)    32          expanded_conv_project[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand (Conv2D)         (None, 16, 16, 48)   384         expanded_conv_project_BN[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_BN (BatchNormali (None, 16, 16, 48)   192         block_1_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_relu (ReLU)      (None, 16, 16, 48)   0           block_1_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_pad (ZeroPadding2D)     (None, 17, 17, 48)   0           block_1_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise (DepthwiseCon (None, 8, 8, 48)     432         block_1_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_BN (BatchNorm (None, 8, 8, 48)     192         block_1_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_relu (ReLU)   (None, 8, 8, 48)     0           block_1_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project (Conv2D)        (None, 8, 8, 16)     768         block_1_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project_BN (BatchNormal (None, 8, 8, 16)     64          block_1_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand (Conv2D)         (None, 8, 8, 96)     1536        block_1_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_BN (BatchNormali (None, 8, 8, 96)     384         block_2_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_relu (ReLU)      (None, 8, 8, 96)     0           block_2_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise (DepthwiseCon (None, 8, 8, 96)     864         block_2_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_BN (BatchNorm (None, 8, 8, 96)     384         block_2_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_relu (ReLU)   (None, 8, 8, 96)     0           block_2_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project (Conv2D)        (None, 8, 8, 16)     1536        block_2_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project_BN (BatchNormal (None, 8, 8, 16)     64          block_2_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_add (Add)               (None, 8, 8, 16)     0           block_1_project_BN[0][0]         \n",
            "                                                                 block_2_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand (Conv2D)         (None, 8, 8, 96)     1536        block_2_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_BN (BatchNormali (None, 8, 8, 96)     384         block_3_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_relu (ReLU)      (None, 8, 8, 96)     0           block_3_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_pad (ZeroPadding2D)     (None, 9, 9, 96)     0           block_3_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise (DepthwiseCon (None, 4, 4, 96)     864         block_3_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_BN (BatchNorm (None, 4, 4, 96)     384         block_3_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_relu (ReLU)   (None, 4, 4, 96)     0           block_3_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project (Conv2D)        (None, 4, 4, 16)     1536        block_3_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project_BN (BatchNormal (None, 4, 4, 16)     64          block_3_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand (Conv2D)         (None, 4, 4, 96)     1536        block_3_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_BN (BatchNormali (None, 4, 4, 96)     384         block_4_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_relu (ReLU)      (None, 4, 4, 96)     0           block_4_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise (DepthwiseCon (None, 4, 4, 96)     864         block_4_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_BN (BatchNorm (None, 4, 4, 96)     384         block_4_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_relu (ReLU)   (None, 4, 4, 96)     0           block_4_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project (Conv2D)        (None, 4, 4, 16)     1536        block_4_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project_BN (BatchNormal (None, 4, 4, 16)     64          block_4_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_add (Add)               (None, 4, 4, 16)     0           block_3_project_BN[0][0]         \n",
            "                                                                 block_4_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand (Conv2D)         (None, 4, 4, 96)     1536        block_4_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_BN (BatchNormali (None, 4, 4, 96)     384         block_5_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_relu (ReLU)      (None, 4, 4, 96)     0           block_5_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise (DepthwiseCon (None, 4, 4, 96)     864         block_5_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_BN (BatchNorm (None, 4, 4, 96)     384         block_5_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_relu (ReLU)   (None, 4, 4, 96)     0           block_5_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project (Conv2D)        (None, 4, 4, 16)     1536        block_5_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project_BN (BatchNormal (None, 4, 4, 16)     64          block_5_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_5_add (Add)               (None, 4, 4, 16)     0           block_4_add[0][0]                \n",
            "                                                                 block_5_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand (Conv2D)         (None, 4, 4, 96)     1536        block_5_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand_BN (BatchNormali (None, 4, 4, 96)     384         block_6_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand_relu (ReLU)      (None, 4, 4, 96)     0           block_6_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_6_pad (ZeroPadding2D)     (None, 5, 5, 96)     0           block_6_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise (DepthwiseCon (None, 2, 2, 96)     864         block_6_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise_BN (BatchNorm (None, 2, 2, 96)     384         block_6_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise_relu (ReLU)   (None, 2, 2, 96)     0           block_6_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_6_project (Conv2D)        (None, 2, 2, 32)     3072        block_6_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_6_project_BN (BatchNormal (None, 2, 2, 32)     128         block_6_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand (Conv2D)         (None, 2, 2, 192)    6144        block_6_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand_BN (BatchNormali (None, 2, 2, 192)    768         block_7_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand_relu (ReLU)      (None, 2, 2, 192)    0           block_7_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise (DepthwiseCon (None, 2, 2, 192)    1728        block_7_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise_BN (BatchNorm (None, 2, 2, 192)    768         block_7_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise_relu (ReLU)   (None, 2, 2, 192)    0           block_7_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_7_project (Conv2D)        (None, 2, 2, 32)     6144        block_7_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_7_project_BN (BatchNormal (None, 2, 2, 32)     128         block_7_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_7_add (Add)               (None, 2, 2, 32)     0           block_6_project_BN[0][0]         \n",
            "                                                                 block_7_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand (Conv2D)         (None, 2, 2, 192)    6144        block_7_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand_BN (BatchNormali (None, 2, 2, 192)    768         block_8_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand_relu (ReLU)      (None, 2, 2, 192)    0           block_8_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise (DepthwiseCon (None, 2, 2, 192)    1728        block_8_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise_BN (BatchNorm (None, 2, 2, 192)    768         block_8_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise_relu (ReLU)   (None, 2, 2, 192)    0           block_8_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_8_project (Conv2D)        (None, 2, 2, 32)     6144        block_8_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_8_project_BN (BatchNormal (None, 2, 2, 32)     128         block_8_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_8_add (Add)               (None, 2, 2, 32)     0           block_7_add[0][0]                \n",
            "                                                                 block_8_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand (Conv2D)         (None, 2, 2, 192)    6144        block_8_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand_BN (BatchNormali (None, 2, 2, 192)    768         block_9_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand_relu (ReLU)      (None, 2, 2, 192)    0           block_9_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise (DepthwiseCon (None, 2, 2, 192)    1728        block_9_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise_BN (BatchNorm (None, 2, 2, 192)    768         block_9_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise_relu (ReLU)   (None, 2, 2, 192)    0           block_9_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_9_project (Conv2D)        (None, 2, 2, 32)     6144        block_9_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_9_project_BN (BatchNormal (None, 2, 2, 32)     128         block_9_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_9_add (Add)               (None, 2, 2, 32)     0           block_8_add[0][0]                \n",
            "                                                                 block_9_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand (Conv2D)        (None, 2, 2, 192)    6144        block_9_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand_BN (BatchNormal (None, 2, 2, 192)    768         block_10_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand_relu (ReLU)     (None, 2, 2, 192)    0           block_10_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise (DepthwiseCo (None, 2, 2, 192)    1728        block_10_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise_BN (BatchNor (None, 2, 2, 192)    768         block_10_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise_relu (ReLU)  (None, 2, 2, 192)    0           block_10_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_10_project (Conv2D)       (None, 2, 2, 48)     9216        block_10_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_10_project_BN (BatchNorma (None, 2, 2, 48)     192         block_10_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand (Conv2D)        (None, 2, 2, 288)    13824       block_10_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand_BN (BatchNormal (None, 2, 2, 288)    1152        block_11_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand_relu (ReLU)     (None, 2, 2, 288)    0           block_11_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise (DepthwiseCo (None, 2, 2, 288)    2592        block_11_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise_BN (BatchNor (None, 2, 2, 288)    1152        block_11_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise_relu (ReLU)  (None, 2, 2, 288)    0           block_11_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_11_project (Conv2D)       (None, 2, 2, 48)     13824       block_11_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_11_project_BN (BatchNorma (None, 2, 2, 48)     192         block_11_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_11_add (Add)              (None, 2, 2, 48)     0           block_10_project_BN[0][0]        \n",
            "                                                                 block_11_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand (Conv2D)        (None, 2, 2, 288)    13824       block_11_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand_BN (BatchNormal (None, 2, 2, 288)    1152        block_12_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand_relu (ReLU)     (None, 2, 2, 288)    0           block_12_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise (DepthwiseCo (None, 2, 2, 288)    2592        block_12_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise_BN (BatchNor (None, 2, 2, 288)    1152        block_12_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise_relu (ReLU)  (None, 2, 2, 288)    0           block_12_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_12_project (Conv2D)       (None, 2, 2, 48)     13824       block_12_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_12_project_BN (BatchNorma (None, 2, 2, 48)     192         block_12_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_12_add (Add)              (None, 2, 2, 48)     0           block_11_add[0][0]               \n",
            "                                                                 block_12_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand (Conv2D)        (None, 2, 2, 288)    13824       block_12_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand_BN (BatchNormal (None, 2, 2, 288)    1152        block_13_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand_relu (ReLU)     (None, 2, 2, 288)    0           block_13_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_13_pad (ZeroPadding2D)    (None, 3, 3, 288)    0           block_13_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise (DepthwiseCo (None, 1, 1, 288)    2592        block_13_pad[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise_BN (BatchNor (None, 1, 1, 288)    1152        block_13_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise_relu (ReLU)  (None, 1, 1, 288)    0           block_13_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_13_project (Conv2D)       (None, 1, 1, 80)     23040       block_13_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_13_project_BN (BatchNorma (None, 1, 1, 80)     320         block_13_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand (Conv2D)        (None, 1, 1, 480)    38400       block_13_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand_BN (BatchNormal (None, 1, 1, 480)    1920        block_14_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand_relu (ReLU)     (None, 1, 1, 480)    0           block_14_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise (DepthwiseCo (None, 1, 1, 480)    4320        block_14_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise_BN (BatchNor (None, 1, 1, 480)    1920        block_14_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise_relu (ReLU)  (None, 1, 1, 480)    0           block_14_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_14_project (Conv2D)       (None, 1, 1, 80)     38400       block_14_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_14_project_BN (BatchNorma (None, 1, 1, 80)     320         block_14_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_14_add (Add)              (None, 1, 1, 80)     0           block_13_project_BN[0][0]        \n",
            "                                                                 block_14_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand (Conv2D)        (None, 1, 1, 480)    38400       block_14_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand_BN (BatchNormal (None, 1, 1, 480)    1920        block_15_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand_relu (ReLU)     (None, 1, 1, 480)    0           block_15_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise (DepthwiseCo (None, 1, 1, 480)    4320        block_15_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise_BN (BatchNor (None, 1, 1, 480)    1920        block_15_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise_relu (ReLU)  (None, 1, 1, 480)    0           block_15_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_15_project (Conv2D)       (None, 1, 1, 80)     38400       block_15_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_15_project_BN (BatchNorma (None, 1, 1, 80)     320         block_15_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_15_add (Add)              (None, 1, 1, 80)     0           block_14_add[0][0]               \n",
            "                                                                 block_15_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand (Conv2D)        (None, 1, 1, 480)    38400       block_15_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand_BN (BatchNormal (None, 1, 1, 480)    1920        block_16_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand_relu (ReLU)     (None, 1, 1, 480)    0           block_16_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise (DepthwiseCo (None, 1, 1, 480)    4320        block_16_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise_BN (BatchNor (None, 1, 1, 480)    1920        block_16_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise_relu (ReLU)  (None, 1, 1, 480)    0           block_16_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_16_project (Conv2D)       (None, 1, 1, 160)    76800       block_16_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_16_project_BN (BatchNorma (None, 1, 1, 160)    640         block_16_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1 (Conv2D)                 (None, 1, 1, 1280)   204800      block_16_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1_bn (BatchNormalization)  (None, 1, 1, 1280)   5120        Conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "out_relu (ReLU)                 (None, 1, 1, 1280)   0           Conv_1_bn[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Logits (Dense)                  (None, 100)          128100      global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 834,324\n",
            "Trainable params: 815,780\n",
            "Non-trainable params: 18,544\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boIkuh4Hq22x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4afdb6e-30c0-4406-b004-41d0ae591284"
      },
      "source": [
        "model.fit(\n",
        "    x_train.astype(np.float32), \n",
        "    y_train.astype(np.float32),\n",
        "    epochs=200,\n",
        "    batch_size=1024,\n",
        "    validation_data=(x_val.astype(np.float32), y_val.astype(np.float32)),\n",
        "    validation_freq=20\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0719 19:38:47.517142 140547230496640 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py:411: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "39/39 [==============================] - 17s 442ms/step - loss: 4.6827 - acc: 0.0173\n",
            "Epoch 2/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 4.4305 - acc: 0.0347\n",
            "Epoch 3/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 4.2177 - acc: 0.0560\n",
            "Epoch 4/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 4.0490 - acc: 0.0761\n",
            "Epoch 5/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 3.9210 - acc: 0.0943\n",
            "Epoch 6/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 3.8026 - acc: 0.1108\n",
            "Epoch 7/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 3.7013 - acc: 0.1262\n",
            "Epoch 8/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 3.5921 - acc: 0.1447\n",
            "Epoch 9/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 3.5036 - acc: 0.1603\n",
            "Epoch 10/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 3.4152 - acc: 0.1773\n",
            "Epoch 11/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 3.3390 - acc: 0.1883\n",
            "Epoch 12/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 3.2641 - acc: 0.2006\n",
            "Epoch 13/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 3.2000 - acc: 0.2134\n",
            "Epoch 14/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 3.1423 - acc: 0.2256\n",
            "Epoch 15/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 3.0696 - acc: 0.2334\n",
            "Epoch 16/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 3.0228 - acc: 0.2441\n",
            "Epoch 17/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 2.9530 - acc: 0.2566\n",
            "Epoch 18/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 2.8966 - acc: 0.2661\n",
            "Epoch 19/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 2.8449 - acc: 0.2769\n",
            "Epoch 20/200\n",
            "9/9 [==============================] - 25s 3s/step\n",
            "9/9 [==============================] - 25s 3s/step\n",
            "39/39 [==============================] - 41s 1s/step - loss: 2.7835 - acc: 0.2891 - val_loss: 4.6919 - val_acc: 0.0097\n",
            "Epoch 21/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 2.7511 - acc: 0.2953\n",
            "Epoch 22/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.6893 - acc: 0.3081\n",
            "Epoch 23/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.6361 - acc: 0.3146\n",
            "Epoch 24/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.5880 - acc: 0.3255\n",
            "Epoch 25/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 2.5314 - acc: 0.3379\n",
            "Epoch 26/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.4899 - acc: 0.3432\n",
            "Epoch 27/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.4538 - acc: 0.3512\n",
            "Epoch 28/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.4072 - acc: 0.3611\n",
            "Epoch 29/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.3455 - acc: 0.3780\n",
            "Epoch 30/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.3076 - acc: 0.3840\n",
            "Epoch 31/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.2565 - acc: 0.3935\n",
            "Epoch 32/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 2.2057 - acc: 0.4048\n",
            "Epoch 33/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.1507 - acc: 0.4168\n",
            "Epoch 34/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 2.1191 - acc: 0.4256\n",
            "Epoch 35/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 2.0620 - acc: 0.4363\n",
            "Epoch 36/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 2.0368 - acc: 0.4396\n",
            "Epoch 37/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 2.0114 - acc: 0.4479\n",
            "Epoch 38/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.9530 - acc: 0.4590\n",
            "Epoch 39/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.9116 - acc: 0.4711\n",
            "Epoch 40/200\n",
            "9/9 [==============================] - 25s 3s/step\n",
            "9/9 [==============================] - 25s 3s/step\n",
            "39/39 [==============================] - 42s 1s/step - loss: 1.8604 - acc: 0.4797 - val_loss: 5.1067 - val_acc: 0.0097\n",
            "Epoch 41/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 1.8136 - acc: 0.4937\n",
            "Epoch 42/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.8063 - acc: 0.4933\n",
            "Epoch 43/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.7552 - acc: 0.5042\n",
            "Epoch 44/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.7134 - acc: 0.5188\n",
            "Epoch 45/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.6831 - acc: 0.5220\n",
            "Epoch 46/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 1.6193 - acc: 0.5392\n",
            "Epoch 47/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.5904 - acc: 0.5477\n",
            "Epoch 48/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.5544 - acc: 0.5572\n",
            "Epoch 49/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 1.5280 - acc: 0.5618\n",
            "Epoch 50/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 1.4778 - acc: 0.5736\n",
            "Epoch 51/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 1.4414 - acc: 0.5828\n",
            "Epoch 52/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 1.4149 - acc: 0.5897\n",
            "Epoch 53/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.4126 - acc: 0.5884\n",
            "Epoch 54/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.3582 - acc: 0.6064\n",
            "Epoch 55/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.3265 - acc: 0.6147\n",
            "Epoch 56/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.3003 - acc: 0.6200\n",
            "Epoch 57/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.2484 - acc: 0.6317\n",
            "Epoch 58/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.2420 - acc: 0.6363\n",
            "Epoch 59/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 1.1995 - acc: 0.6454\n",
            "Epoch 60/200\n",
            "9/9 [==============================] - 27s 3s/step\n",
            "9/9 [==============================] - 27s 3s/step\n",
            "39/39 [==============================] - 45s 1s/step - loss: 1.1748 - acc: 0.6516 - val_loss: 5.8901 - val_acc: 0.0097\n",
            "Epoch 61/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 1.1251 - acc: 0.6653\n",
            "Epoch 62/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.0899 - acc: 0.6766\n",
            "Epoch 63/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.0712 - acc: 0.6804\n",
            "Epoch 64/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.0360 - acc: 0.6924\n",
            "Epoch 65/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 1.0328 - acc: 0.6931\n",
            "Epoch 66/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 1.0075 - acc: 0.6991\n",
            "Epoch 67/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.9826 - acc: 0.7085\n",
            "Epoch 68/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.9588 - acc: 0.7105\n",
            "Epoch 69/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.9429 - acc: 0.7144\n",
            "Epoch 70/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.9102 - acc: 0.7270\n",
            "Epoch 71/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.8897 - acc: 0.7304\n",
            "Epoch 72/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.8619 - acc: 0.7399\n",
            "Epoch 73/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.8413 - acc: 0.7451\n",
            "Epoch 74/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.8324 - acc: 0.7479\n",
            "Epoch 75/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.7928 - acc: 0.7601\n",
            "Epoch 76/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.7596 - acc: 0.7677\n",
            "Epoch 77/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.7509 - acc: 0.7721\n",
            "Epoch 78/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.7439 - acc: 0.7736\n",
            "Epoch 79/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.7484 - acc: 0.7714\n",
            "Epoch 80/200\n",
            "9/9 [==============================] - 28s 3s/step\n",
            "9/9 [==============================] - 28s 3s/step\n",
            "39/39 [==============================] - 47s 1s/step - loss: 0.7302 - acc: 0.7774 - val_loss: 6.7157 - val_acc: 0.0097\n",
            "Epoch 81/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.6928 - acc: 0.7910\n",
            "Epoch 82/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.6859 - acc: 0.7891\n",
            "Epoch 83/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.6707 - acc: 0.7944\n",
            "Epoch 84/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.7056 - acc: 0.7837\n",
            "Epoch 85/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.6875 - acc: 0.7872\n",
            "Epoch 86/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.6502 - acc: 0.7999\n",
            "Epoch 87/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.6389 - acc: 0.8037\n",
            "Epoch 88/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.6041 - acc: 0.8167\n",
            "Epoch 89/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.6062 - acc: 0.8156\n",
            "Epoch 90/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.5785 - acc: 0.8228\n",
            "Epoch 91/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.5925 - acc: 0.8203\n",
            "Epoch 92/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.5709 - acc: 0.8257\n",
            "Epoch 93/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.5606 - acc: 0.8277\n",
            "Epoch 94/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.5375 - acc: 0.8366\n",
            "Epoch 95/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.5311 - acc: 0.8390\n",
            "Epoch 96/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.5229 - acc: 0.8415\n",
            "Epoch 97/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.5169 - acc: 0.8402\n",
            "Epoch 98/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.5167 - acc: 0.8410\n",
            "Epoch 99/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.5022 - acc: 0.8466\n",
            "Epoch 100/200\n",
            "9/9 [==============================] - 29s 3s/step\n",
            "9/9 [==============================] - 29s 3s/step\n",
            "39/39 [==============================] - 48s 1s/step - loss: 0.4890 - acc: 0.8515 - val_loss: 7.3100 - val_acc: 0.0086\n",
            "Epoch 101/200\n",
            "39/39 [==============================] - 1s 37ms/step - loss: 0.4861 - acc: 0.8510\n",
            "Epoch 102/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.5021 - acc: 0.8445\n",
            "Epoch 103/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.5032 - acc: 0.8444\n",
            "Epoch 104/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.4941 - acc: 0.8464\n",
            "Epoch 105/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.4851 - acc: 0.8496\n",
            "Epoch 106/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.4700 - acc: 0.8562\n",
            "Epoch 107/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.4611 - acc: 0.8578\n",
            "Epoch 108/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.4567 - acc: 0.8600\n",
            "Epoch 109/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.4456 - acc: 0.8631\n",
            "Epoch 110/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.4296 - acc: 0.8669\n",
            "Epoch 111/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.4336 - acc: 0.8668\n",
            "Epoch 112/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.4228 - acc: 0.8701\n",
            "Epoch 113/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.4244 - acc: 0.8693\n",
            "Epoch 114/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.4040 - acc: 0.8755\n",
            "Epoch 115/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.3980 - acc: 0.8766\n",
            "Epoch 116/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3789 - acc: 0.8837\n",
            "Epoch 117/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.3968 - acc: 0.8775\n",
            "Epoch 118/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.4047 - acc: 0.8753\n",
            "Epoch 119/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3891 - acc: 0.8801\n",
            "Epoch 120/200\n",
            "9/9 [==============================] - 32s 4s/step\n",
            "9/9 [==============================] - 32s 4s/step\n",
            "39/39 [==============================] - 52s 1s/step - loss: 0.3832 - acc: 0.8824 - val_loss: 6.6248 - val_acc: 0.0240\n",
            "Epoch 121/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.4007 - acc: 0.8758\n",
            "Epoch 122/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3751 - acc: 0.8848\n",
            "Epoch 123/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.3693 - acc: 0.8862\n",
            "Epoch 124/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.3638 - acc: 0.8879\n",
            "Epoch 125/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3659 - acc: 0.8863\n",
            "Epoch 126/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3567 - acc: 0.8898\n",
            "Epoch 127/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3756 - acc: 0.8850\n",
            "Epoch 128/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.3810 - acc: 0.8820\n",
            "Epoch 129/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3703 - acc: 0.8868\n",
            "Epoch 130/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3406 - acc: 0.8957\n",
            "Epoch 131/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.3418 - acc: 0.8933\n",
            "Epoch 132/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3314 - acc: 0.8983\n",
            "Epoch 133/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3218 - acc: 0.9001\n",
            "Epoch 134/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.3349 - acc: 0.8968\n",
            "Epoch 135/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3243 - acc: 0.8996\n",
            "Epoch 136/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3309 - acc: 0.8962\n",
            "Epoch 137/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3506 - acc: 0.8908\n",
            "Epoch 138/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.3271 - acc: 0.8971\n",
            "Epoch 139/200\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.3240 - acc: 0.9007\n",
            "Epoch 140/200\n",
            "9/9 [==============================] - 33s 4s/step\n",
            "9/9 [==============================] - 33s 4s/step\n",
            "39/39 [==============================] - 56s 1s/step - loss: 0.3282 - acc: 0.8988 - val_loss: 5.3676 - val_acc: 0.1432\n",
            "Epoch 141/200\n",
            "39/39 [==============================] - 1s 37ms/step - loss: 0.3234 - acc: 0.8993\n",
            "Epoch 142/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.3111 - acc: 0.9042\n",
            "Epoch 143/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3152 - acc: 0.9029\n",
            "Epoch 144/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3117 - acc: 0.9036\n",
            "Epoch 145/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3186 - acc: 0.9010\n",
            "Epoch 146/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.3336 - acc: 0.8958\n",
            "Epoch 147/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2994 - acc: 0.9074\n",
            "Epoch 148/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3110 - acc: 0.9020\n",
            "Epoch 149/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3098 - acc: 0.9042\n",
            "Epoch 150/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.3020 - acc: 0.9060\n",
            "Epoch 151/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.3100 - acc: 0.9049\n",
            "Epoch 152/200\n",
            "39/39 [==============================] - 1s 37ms/step - loss: 0.2970 - acc: 0.9078\n",
            "Epoch 153/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2711 - acc: 0.9166\n",
            "Epoch 154/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2924 - acc: 0.9101\n",
            "Epoch 155/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2766 - acc: 0.9133\n",
            "Epoch 156/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2597 - acc: 0.9193\n",
            "Epoch 157/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2755 - acc: 0.9153\n",
            "Epoch 158/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2849 - acc: 0.9115\n",
            "Epoch 159/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2832 - acc: 0.9123\n",
            "Epoch 160/200\n",
            "9/9 [==============================] - 35s 4s/step\n",
            "9/9 [==============================] - 35s 4s/step\n",
            "39/39 [==============================] - 59s 2s/step - loss: 0.2840 - acc: 0.9128 - val_loss: 8.0129 - val_acc: 0.1418\n",
            "Epoch 161/200\n",
            "39/39 [==============================] - 1s 37ms/step - loss: 0.2884 - acc: 0.9092\n",
            "Epoch 162/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2965 - acc: 0.9085\n",
            "Epoch 163/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.3120 - acc: 0.9032\n",
            "Epoch 164/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2808 - acc: 0.9121\n",
            "Epoch 165/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2881 - acc: 0.9086\n",
            "Epoch 166/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2871 - acc: 0.9108\n",
            "Epoch 167/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2885 - acc: 0.9094\n",
            "Epoch 168/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2725 - acc: 0.9153\n",
            "Epoch 169/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2698 - acc: 0.9159\n",
            "Epoch 170/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2682 - acc: 0.9169\n",
            "Epoch 171/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2682 - acc: 0.9161\n",
            "Epoch 172/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2681 - acc: 0.9179\n",
            "Epoch 173/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2573 - acc: 0.9201\n",
            "Epoch 174/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2490 - acc: 0.9210\n",
            "Epoch 175/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2513 - acc: 0.9230\n",
            "Epoch 176/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2289 - acc: 0.9293\n",
            "Epoch 177/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2513 - acc: 0.9226\n",
            "Epoch 178/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2594 - acc: 0.9187\n",
            "Epoch 179/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2421 - acc: 0.9241\n",
            "Epoch 180/200\n",
            "9/9 [==============================] - 35s 4s/step\n",
            "9/9 [==============================] - 35s 4s/step\n",
            "39/39 [==============================] - 57s 1s/step - loss: 0.2697 - acc: 0.9162 - val_loss: 11.7364 - val_acc: 0.1188\n",
            "Epoch 181/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2699 - acc: 0.9144\n",
            "Epoch 182/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2490 - acc: 0.9222\n",
            "Epoch 183/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2558 - acc: 0.9193\n",
            "Epoch 184/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2447 - acc: 0.9232\n",
            "Epoch 185/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2397 - acc: 0.9252\n",
            "Epoch 186/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2547 - acc: 0.9201\n",
            "Epoch 187/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2525 - acc: 0.9210\n",
            "Epoch 188/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2409 - acc: 0.9254\n",
            "Epoch 189/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2429 - acc: 0.9248\n",
            "Epoch 190/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2535 - acc: 0.9218\n",
            "Epoch 191/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2306 - acc: 0.9268\n",
            "Epoch 192/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2464 - acc: 0.9230\n",
            "Epoch 193/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2404 - acc: 0.9250\n",
            "Epoch 194/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2275 - acc: 0.9284\n",
            "Epoch 195/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2242 - acc: 0.9301\n",
            "Epoch 196/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2325 - acc: 0.9272\n",
            "Epoch 197/200\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.2293 - acc: 0.9281\n",
            "Epoch 198/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2231 - acc: 0.9298\n",
            "Epoch 199/200\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.2218 - acc: 0.9305\n",
            "Epoch 200/200\n",
            "9/9 [==============================] - 37s 4s/step\n",
            "9/9 [==============================] - 37s 4s/step\n",
            "39/39 [==============================] - 63s 2s/step - loss: 0.2115 - acc: 0.9341 - val_loss: 14.8284 - val_acc: 0.1017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd35c869d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pTyj1D1Rutk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.predict(x_train[:8].astype(np.float32), batch_size=8, verbose=0, steps=None, callbacks=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ck-MUuyz6LH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8768b7b-5704-4c06-9b94-7717c9239504"
      },
      "source": [
        "len(results)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubCN8-tH9mit",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b900c7b-c65b-431d-ab66-9f147454dd56"
      },
      "source": [
        "results.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NmEGOoQ9ovG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}